# transformer-training

Train a GPT2 model from scratch using shakespeare character encoding instead of word-level encoding. 

Leveraged repo: https://github.com/karpathy/nanoGPT
